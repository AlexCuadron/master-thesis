\chapter{Diagnosing Overthinking}
\label{overthinking}

This chapter presents our framework for identifying and analyzing overthinking behavior in Large Reasoning Models (LRMs). We begin by defining the Reasoning-Action Dilemma that leads to overthinking, then describe the manifestations of this behavior, and finally present our methodology for quantifying it. Through our analysis of 3,908 trajectories, we demonstrate that overthinking represents a significant challenge in agentic environments, particularly affecting models specifically trained for reasoning tasks.

\section{The Reasoning-Action Dilemma}
\label{sec:dilemma_detailed}

We observe that, in agentic decision-making tasks, LRMs constantly face the \emph{Reasoning–Action Dilemma}. This fundamental trade-off requires models to balance two competing approaches: direct interaction with the environment through action execution and feedback collection, and internal reasoning where models simulate hypothetical outcomes before committing to actions. This dilemma is particularly challenging because both approaches have their merits—environmental interaction provides ground truth but can be costly or risky, while internal reasoning allows for safe exploration but may diverge from reality.

Ideally, an LRM should balance reasoning and action by using internal simulation to refine its choices while leveraging real-world feedback to correct errors. For instance, when debugging a failing test case, a well-balanced model would hypothesize potential issues yet still execute the test opportunely to collect concrete failure signals. This balance is particularly critical in software engineering tasks, where models must understand complex codebases, reason about potential fixes, and validate their solutions through testing.

Unfortunately, achieving this balance is inherently challenging. In such settings, feedback is sparse, compelling LRMs to rely heavily on internal simulation to make the most out of every iteration. However, prior research has demonstrated that LRMs exhibit significant vulnerability to knowledge insufficiency, where gaps in understanding can cascade into compounding errors throughout the reasoning process \cite{li2025searcho1agenticsearchenhancedlarge, zhong2024evaluationopenaio1opportunities, LingFLHLMS23, chia2024reasoningpathsoptimizationlearning}. This challenge is exacerbated by LRMs' sensitivity to prompt modifications \cite{openai_learning_to_reason_2024, deepseekai2025deepseekr1incentivizingreasoningcapability}, which can disrupt their ability to maintain and process contextual information. These limitations become especially critical in agentic scenarios, where models must simultaneously gather, retain, and act upon new information \cite{zhang2024agenticinformationretrieval, yang2024sweagentagentcomputerinterfacesenable}. Consequently, excessive simulation without sufficient external information can ultimately lead to failure. The situation is especially difficult for environments with limited interaction opportunities.

\section{Manifestations of Overthinking}
\label{sec:manifestations_detailed}

Our investigation into impaired decision-making in AI agents draws from a detailed analysis of agent-environment interactions. These interactions are recorded in what we term trajectories - comprehensive logs that capture the complete sequence of agent actions, environment responses, and (where available) the agent's reasoning process. As outlined in \cref{evaluation_framework}, we systematically analyzed these trajectories to understand patterns of \textbf{overthinking}.

While most trajectories include the agent's explicit reasoning process, those from the o1 family exclude these reasoning tokens \cite{openai_learning_to_reason_2024}. This limitation led us to focus our analysis on observable behaviors, which are the concrete actions agents take in response to environmental challenges.

Through this analysis, we identified three distinct patterns of \textbf{overthinking}, as illustrated in \cref{fig:manifestations}:

\subsection{Analysis Paralysis}
When faced with challenges, LRMs tend to shift their focus from immediate actions to elaborate future planning. They generate increasingly complex action sequences but struggle to execute them systematically. Rather than addressing immediate errors, they construct intricate plans that often remain unexecuted, leading to a cycle of planning without progress. This behavior is particularly evident in software engineering tasks, where models may spend excessive time analyzing potential code changes without actually implementing and testing them.

\subsection{Rogue Actions}
We observe cases where agents deliberately try to bypass environmental exploration by generating chains of interdependent actions in a single step. Despite their prior demonstrated awareness of step-by-step interaction requirements, models proceed to construct elaborate action sequences that presume the success of each preceding step, effectively substituting real environmental feedback with internal simulation. This behavior often manifests as attempts to execute multiple file modifications or system commands simultaneously, without waiting for confirmation of each step's success.

\subsection{Premature Disengagement}
LRMs sometimes terminate tasks based solely on their internal simulation of the problem space, either through direct abandonment or by delegating hypothetical action sequences. This illustrates how overreliance on internal reasoning can lead to decisions without environmental validation. For example, a model might declare a bug fixed based on its analysis of the code changes, without actually running the tests to verify the fix works as intended.

\section{Quantifying Overthinking}
\label{sec:quantifying}

To systematically study overthinking in LRMs, we developed a comprehensive evaluation framework that combines automated analysis with human validation. Our approach focuses on quantifying the degree to which models favor internal reasoning over environmental interaction, allowing us to measure the impact of overthinking on model performance and identify potential mitigation strategies. Through this framework, we analyzed 3,908 trajectories across 19 different models, revealing significant patterns in how overthinking manifests and affects model behavior in agentic environments.

\subsection{Overthinking Score}
To quantify overthinking behavior, we developed a systematic scoring method using an LLM-based evaluator. This evaluator analyzes model trajectories for the previously described patterns and assigns a score from 0 to 10, with higher scores indicating more severe overthinking behavior. Each score includes a detailed justification explaining which patterns were identified and their severity. The complete evaluation prompt and scoring criteria can be found in \cref{apx:prompt_overthinking}.

Our analysis reveals two distinct patterns in overthinking behavior. First, regression analysis demonstrates a significant negative correlation between overthinking and issue resolution rates for both reasoning and non-reasoning models (\cref{fig:figure1}), with the latter showing a steeper decline in performance as overthinking increases. Second, a direct comparison reveals that reasoning models consistently exhibit higher overthinking scores—nearly three times higher than non-reasoning models—with this difference being statistically significant as shown in \cref{tab:overthinking_scores}.

\subsection{Validation Methodology}
To validate our LLM-based evaluator, we conducted an independent assessment where four expert annotators manually scored 20 randomly selected model traces. The Spearman rank correlation ($p = 0.800$) between human experts and our automated scoring system demonstrated a strong monotonic relationship, as shown in \cref{fig:human_eval}. This non-parametric correlation measure confirms the reliability of our overthinking measurement methodology.

\subsection{Impact on Model Performance}
Statistical analysis reveals that higher overthinking scores correlate with decreased performance across all models. This correlation is particularly strong for reasoning models, which exhibit consistently higher overthinking tendencies, suggesting that excessive reliance on internal simulation impairs task performance. As illustrated in \cref{fig:figure1}, model nomenclature such as "FC" (indicating native function calling capability), "DS" (representing DeepSeek models), and suffixes o1\_high and o1\_low denoting models with reasoning effort set to high and low respectively, all show this consistent pattern.

\subsection{Practical Implications}
Our findings demonstrate that simple efforts to mitigate overthinking can yield substantial benefits. By generating two candidate solutions using o1 with low reasoning effort and selecting the one with lower overthinking score, we not only improved issue resolution from 21\% to 26.3\% on SWE Bench Verified tasks (\cref{fig:figure2}), but also nearly matched the performance of high-reasoning configurations while \textbf{reducing computational costs by approximately 43\%}. This demonstrates that simple overthinking mitigation strategies can dramatically improve the efficiency of LRMs in real-world applications.

\section{Evaluation Framework}
\label{sec:eval_framework}

Our evaluation framework consists of several key components designed to systematically analyze overthinking behavior in LRMs. We employ SWE Bench Verified \cite{jimenez2024swebenchlanguagemodelsresolve, swebench_verified} as our benchmark, using the CodeAct agent scaffolding \cite{wang2024executablecodeactionselicit} within the OpenHands framework \cite{wang2024openhandsopenplatformai}. This setup creates a controlled environment where models must balance information gathering with reasoning chains while maintaining context across multiple interactions as illustrated in \autoref{fig:figure3}.

\subsection{Models Evaluated}
To comprehensively study the phenomenon and influence of overthinking, we evaluated 19 models selected to represent diverse characteristics across the current LLM landscape. Our selection spans both reasoning-optimized models and general-purpose language models to understand how specialized reasoning training affects overthinking tendencies. We included both proprietary models (such as OpenAI's o1 and Claude Sonnet 3.5) and open-source alternatives (like DeepSeek-R1 and Qwen2.5) to ensure our findings generalize across different development approaches. The models range in size from relatively small (1.5B-14B parameters) to large-scale architectures (32B-671B parameters), allowing us to investigate how model scale influences overthinking behavior. Additionally, we compared models with native function calling capabilities (such as OpenAI o1 and GPT-4o) against those without, to understand how explicit tool-use training affects environmental interaction patterns.

\subsection{Trajectory Analysis}
Our analysis centers on comprehensive interaction logs that we term trajectories. Each trajectory captures the complete temporal sequence of model actions, including precise timing information, along with the corresponding environmental responses. Where available, we also record the model's internal reasoning process, though this is notably absent in some models like the o1 family. These logs culminate in clear success or failure indicators for each task attempt, providing objective metrics for performance evaluation.

Through our analysis of \textbf{3,908 trajectories}, we have created a comprehensive open-source dataset to advance research in balancing reasoning and action in agentic environments. Each trajectory is accompanied by its overthinking score and detailed justification, enabling future researchers to build upon our findings and develop improved strategies for managing the reasoning-action trade-off.

\subsection{Pattern Recognition}
Our framework employs a systematic approach to identify key behavioral patterns indicative of overthinking. We focus particularly on extended reasoning chains that lack corresponding actions, as these often signal a model's excessive reliance on internal simulation. We also track instances where models attempt to generate multiple actions in a single turn, bypassing the intended step-by-step interaction process. Additionally, we monitor for premature task termination and cases where models disregard environmental feedback, both of which suggest an overreliance on internal world models rather than real-world interaction.

These patterns are evaluated within the context of software engineering tasks, where we can leverage automated test suites and code validation tools to objectively measure task completion. This approach ensures our pattern recognition is grounded in concrete, measurable outcomes rather than subjective assessments.

\subsection{Statistical Validation}
Our statistical analysis reveals three key findings about overthinking in language models: its relationship with performance in SWE-bench, its non-equal prevalence across model types, and its practical implications for model selection. We employ rigorous statistical methods to ensure the reliability of our findings.

Regression analysis demonstrates a significant negative correlation between overthinking and issue resolution rates for both reasoning and non-reasoning models. Non-reasoning models show a steeper decline in performance as overthinking increases, with a beta coefficient of -25.802 compared to -11.476 for reasoning models. Both relationships are statistically significant (p = 0.031 and p = 0.006 respectively).

A direct comparison reveals that reasoning models consistently exhibit higher overthinking scores—nearly three times higher than non-reasoning models. Reasoning models average 2.320 ± 1.120 on our overthinking scale, while non-reasoning models score 0.865 ± 0.432. This difference is statistically significant (p = 0.019), suggesting a systematic tendency toward overthinking in reasoning-optimized models.

To validate our automated scoring system, we conducted an independent assessment with four expert annotators who manually scored 20 randomly selected model traces. The Spearman rank correlation (p = 0.800) between human experts and our automated system demonstrates a strong monotonic relationship, confirming the reliability of our measurement methodology.

The complete analytical framework and results are presented in \cref{evaluation_framework}. The tools used for the statistical analysis can be found in \cref{stat_framework}.

\section{Implementation Details}
\label{sec:implementation}

\subsection{LLM-as-Judge System}
Following the LLM-as-a-judge methodology \cite{zheng2023judgingllmasajudgemtbenchchatbot}, we developed a specialized evaluation system using Claude Sonnet 3.5 as our judge model. The system is configured for maximum reliability and transparency, with temperature set to 0 to ensure deterministic scoring across evaluations. We leverage Claude's 200k token context window to process complete trajectories alongside detailed evaluation criteria, and enforce a structured output format to maintain consistency across assessments. Each evaluation must include detailed justifications, enabling thorough review and validation of the scoring process.

Claude Sonnet 3.5 was selected specifically for its extensive context window and proven reliability in complex evaluation tasks. To prevent bias, we deliberately withhold final issue resolution outcomes from the evaluator, ensuring that overthinking assessments are based purely on behavioral patterns rather than task success. This approach maintains the independence of our overthinking metrics from performance measures, allowing us to analyze their relationship without circular dependencies.

\subsection{Scoring Criteria}
Our scoring system implements a multi-dimensional evaluation approach that considers several key aspects of model behavior. At its core, we assess how well models balance their reasoning processes with concrete actions, looking for evidence of effective decision-making that combines thoughtful analysis with practical implementation. We evaluate the model's ability to appropriately incorporate environmental feedback into its decision-making process, particularly focusing on how it adapts its approach based on system responses. The efficiency of the problem-solving approach is also considered, examining whether models take unnecessarily complex paths or maintain a focused, goal-oriented strategy. Finally, we track the completion of necessary validation steps, ensuring that models don't skip crucial verification processes in their rush to propose solutions.

\subsection{Model Size and Overthinking}
Our initial hypothesis posited that smaller LRMs would struggle more with environmental comprehension, leading them to rely heavily on internal reasoning chains and thus exhibit increased overthinking behavior. To test this hypothesis, we analyzed two model families across four size variants (32B, 14B, 7B, and 1.5B): the non-reasoning Qwen2.5-Instruct and the reasoning R1-Distill-Qwen.

Our analysis revealed a strong negative correlation between model size and overthinking scores: larger models demonstrated less tendency to overthink, as shown in \cref{figure5}. The distinction between reasoning and non-reasoning models becomes particularly pronounced at smaller scales. R1 models exhibit significantly higher overthinking scores (5.157 ± 2.292) compared to their Qwen2.5 counterparts (1.771 ± 0.463), with this gap widening as model size decreases. This difference is statistically significant (t = 2.508, p = 0.046).

\subsection{Token Usage Analysis}
We investigated the relationship between token consumption and overthinking behavior in the o1 model. We conducted experiments by manipulating the model's reasoning effort parameter between high and low settings, which influences the number of reasoning tokens used \cite{openai_chat_api}. Our analysis revealed an unexpected pattern: in agent-based scenarios, o1 models with low reasoning effort demonstrated 35\% higher overthinking scores compared to their high-effort counterparts. As shown in \cref{tab:o1_model_comparison}, the difference in averaged overthinking scores between the two configurations is statistically significant, suggesting that increased token allocation might paradoxically reduce overthinking in agentic contexts.

\subsection{Function Calling Impact}
Our experimental analysis compared O1 model configurations with high reasoning effort, evaluating performance both with and without native function calling (FC) capabilities. The integration of FC capabilities yielded substantial improvements across multiple metrics. Most notably, we observed a dramatic increase in performance score from 29.1\% to 47.7\%, accompanied by a significant reduction in average overthinking score from 2.47 to 0.56. These improvements suggest that native function calling capabilities help models maintain a more balanced approach to environmental interaction.

However, benchmarking against BCFL \cite{berkeley-function-calling-leaderboard} reveals a more nuanced pattern. In multi-turn environments, the performance differential between FC and non-FC implementations of O1 shows a more modest improvement from 36\% to 41\%. This comparatively smaller enhancement suggests that FC implementation alone cannot fully account for the dramatic performance improvements observed in our primary experiments, indicating that other factors may play significant roles in reducing overthinking behavior.

OpenAI has demonstrated that reasoning models exhibit a disproportionate increase in computational costs relative to their performance gains \cite{arcprize2024oai}. Our experiments with SWE-bench Verified dataset confirm this observation: o1 with high reasoning effort achieves a 29.1\% resolution rate at \$1,400, while the low reasoning variant reaches 21.0\% at \$400—a 3.5x cost difference for an 8.1 percentage point improvement in performance.

To address this efficiency gap, we developed an alternative approach: running the low-reasoning variant twice and selecting traces with minimal overthinking scores. This method achieved a 26.3\% resolution rate while consuming only 57\% of the high-reasoning configuration's cost. Our findings suggest that monitoring and controlling overthinking behavior could be a cost-effective strategy for optimizing model performance in real-world applications.

\section{Limitations and Considerations}
\label{sec:limitations}

While our framework provides valuable insights into overthinking behavior, it has several limitations that inform our ongoing research directions:

\subsection{Technical Limitations}
Our framework, while effective, faces several significant technical constraints. A primary challenge stems from limited access to internal reasoning processes, particularly in models like o1 that exclude reasoning tokens \cite{openai_learning_to_reason_2024}. This limitation forces us to rely primarily on observable behaviors, potentially obscuring important aspects of the overthinking process that manifest in internal model states.

The use of LLM-based evaluation, while showing strong correlation with human judgments, introduces its own set of challenges. While Claude Sonnet 3.5 has proven reliable as an evaluator, its use may introduce systematic biases in overthinking detection that could affect our analysis. This concern is particularly relevant given the complex nature of the behaviors we're trying to quantify.

Resource constraints also pose significant challenges to our approach. Processing and analyzing our dataset of 3,908 trajectories requires substantial computational resources, particularly when evaluating models with high reasoning effort settings. This computational burden becomes especially pronounced when dealing with larger language models or more complex interaction patterns. As we look to expand our analysis to larger datasets or more intricate environmental interactions, these scalability challenges may become increasingly significant.

\subsection{Methodological Considerations}
Our methodology, while systematic, faces several important limitations in its current form. While we've identified three main patterns of overthinking, the complexity of model behavior suggests there may be additional manifestations that our current framework doesn't capture. This potential incompleteness in our pattern identification could affect our ability to fully characterize overthinking behavior in different contexts.

The human validation process presents another significant challenge. Validating overthinking scores requires substantial expert time and attention, with each trajectory requiring careful analysis to ensure accurate assessment. This intensive validation process may not scale well to larger studies, potentially limiting our ability to expand the scope of our analysis.

Domain specificity also presents a methodological concern. Our findings are primarily based on software engineering tasks using SWE Bench Verified, and while these provide a rich environment for studying model behavior, the patterns and implications we've identified may not generalize uniformly to other domains. Different task environments might elicit different manifestations of overthinking, requiring additional validation and potentially different evaluation criteria.

The relationship between model scale and overthinking behavior adds another layer of complexity to our methodology. Our analysis of DeepSeek-R1-671B, which revealed overthinking scores comparable to DeepSeek-V3-671B, suggests that model scale and training methodology interact in ways that are not yet fully understood. This observation highlights the need for more nuanced evaluation approaches that can account for these complex interactions.

\subsection{Future Research Directions}
Our analysis suggests two promising approaches to mitigate overthinking in LRMs. First, the success of function-calling architectures hints at the importance of explicit interaction training, though our BCFL benchmarking indicates this alone isn't sufficient for optimal performance. Second, the effectiveness of limited reinforcement learning in DeepSeek-R1-671B points to the crucial role of training methodology in controlling overthinking tendencies.

These findings raise several critical questions for future research. A primary concern is how these approaches might generalize across different domains, as the effectiveness of mitigation strategies may vary significantly depending on the specific requirements and constraints of each application area. We must also consider how to optimize these approaches for environments where environmental interaction carries varying costs, balancing the benefits of active exploration against operational constraints. Additionally, the role of model scale in overthinking behavior requires further investigation, as our results suggest complex interactions between model size, architecture, and overthinking tendencies.

Understanding these dynamics could help develop more robust solutions that prevent, rather than just mitigate, overthinking behaviors in large reasoning models. To facilitate further research in this direction, we release our evaluation framework and dataset of 3,908 trajectories, enabling the broader research community to build upon these findings across different environments and architectures.

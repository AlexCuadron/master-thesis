\begin{abstract}
        Large Reasoning Models (LRMs) represent a breakthrough in AI problem-solving capabilities, but their effectiveness in interactive environments may be limited. This thesis introduces and analyzes \textbf{overthinking} in LRMs—a phenomenon where models favor extended internal reasoning chains over environmental interaction. Through experiments on software engineering tasks using SWE Bench Verified, we observe three recurring patterns: \textit{Analysis Paralysis, Rogue Actions, and Premature Disengagement}. We propose a framework to study these behaviors, which correlates with human expert assessments, and analyze \textbf{3,908 trajectories}. We observe that higher overthinking scores correlate with decreased performance, with reasoning models exhibiting stronger tendencies toward overthinking compared to non-reasoning models. Our analysis reveals that simple solutions—such as selecting solutions with lower overthinking scores—can \textbf{improve model performance by 25\% while decreasing computational costs by 43\%}. Based on these findings, we suggest promising directions for mitigating overthinking through native function-calling capabilities and selective reinforcement learning, potentially offering a path to better balance reasoning and environmental interaction. To facilitate further research in this direction, we open-source our evaluation framework and dataset.
\end{abstract}

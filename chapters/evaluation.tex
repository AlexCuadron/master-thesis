\chapter{Evaluation}
\label{eval}

To assess the impact of overthinking in Large Reasoning Models, we analyze their performance in agentic environments using SWE-Bench Verified, comparing reasoning models with their non-reasoning counterparts. Our study aims to answer the following research questions:
\begin{itemize}
    \item RQ1: Does overthinking affect performance?
    \item RQ2: Does its impact affect all models equally?
    \item RQ3: How to potentially mitigate overthinking?
\end{itemize}

\section{Experimental Setup}
\label{sec:setup}

\subsection{Models Evaluated}
To comprehensively study the phenomenon and influence of overthinking, we consider 19 models across multiple dimensions, including reasoning capabilities, model openness (proprietary vs. open-source), model size, and function calling support. We evaluate both reasoning-optimized models as well as general-purpose language models. Our evaluation spans proprietary models (e.g., OpenAI o1, Claude Sonnet 3.5)~\cite{openai_learning_to_reason_2024,anthropic_claude_3_5} and open-source alternatives (e.g., DeepSeek-R1, Qwen2.5) \cite{qwen2, qwen2.5, deepseekai2025deepseekr1incentivizingreasoningcapability} to ensure broad coverage.

We also analyze models of varying scale, ranging from small (1.5B-14B) to large-scale models (32B-671B parameters) \cite{deepseek_reasoning_model}, to investigate whether model size influences overthinking tendencies. Additionally, we distinguish between models that natively support function calling (e.g., OpenAI o1, GPT-4o) \cite{openai_function_calling, openai_gpt4o_2024, openai_gpt4o_mini, openai_o1} and those that do not, which allows us to assess whether explicit function calling capabilities reduce overthinking compared to models that rely on prompt-based learning of tool usage. Further details on the models studied can be found in the~\autoref{apx:models},~\autoref{tab:model_comparison}.

\subsection{Scaffoldings}
To enable tool use, we adopt CodeAct, an open-source single-agent scaffolding built within the OpenHands framework~\cite{openhands}, as models alone cannot directly execute code or edit files~\cite{qwq-32b-preview, openai_o1_mini, openai_o1_system_card_2024, deepseekai2025deepseekr1incentivizingreasoningcapability, sky_t1_2025}. Scaffolding provides a structured execution environment, allowing models to interact with SWE-bench in a controlled and consistent manner. We choose the single-agent approach as it maintains a unified reasoning process, ensuring full context retention throughout execution.

In contrast, multi-agent scaffolds distribute tasks across multiple specialized agents that share an underlying model but operate with distinct prompts and action spaces~\cite{chen2024coderissueresolvingmultiagent,xia2024agentlessdemystifyingllmbasedsoftware, phan2024hyperagentgeneralistsoftwareengineering, allhands_single_agent_systems} which can introduce structural rigidity and lead to information loss during inter-agent communication~\cite{allhands_single_agent_systems}. Similarly, since our goal is to evaluate tool use and in-context memory, we avoid OpenAI's Agentless approach~\cite{openai_o1_system_card_2024, xia2024agentlessdemystifyingllmbasedsoftware}, which follows a fixed three-step process (localization, repair, patch validation) without allowing adaptive decision-making. Therefore, we can ensure all models are evaluated in a standardized, interactive environment.

\section{Overthinking Score Calculation}
\label{sec:score_calc}

We devise an LLM-based evaluation framework with a carefully designed prompt to systematically assess overthinking behaviors. This evaluator is instructed to recognize overthinking patterns, as described in~\autoref{sec:manifestations}, within model trajectories in agentic scenarios. Based on the identified patterns, the evaluator assigns an overthinking score on a scale from 0 to 10 and provides a detailed justification for the assigned score.

To ensure reliability and consistency, we employ Claude Sonnet 3.5 as the evaluation model and configure it with a temperature of 0 to enforce deterministic scoring, following the LLM-as-a-judge methodology~\cite{zheng2023judgingllmasajudgemtbenchchatbot}. Claude Sonnet 3.5 was selected for its 200k-token context window, allowing it to process complete trajectories alongside the evaluation criteria.

Notably, the evaluator does not have access to the final issue resolution outcome, ensuring that the overthinking assessment remains independent of task success and thereby eliminating potential biases. The complete evaluation prompt and scoring criteria can be found in \cref{apx:prompt_overthinking}.